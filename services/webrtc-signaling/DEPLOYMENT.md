# WebRTC Signaling Service - Deployment Guide

## Overview

This service has been configured for deployment to Azure Container Apps with the following setup:

- **Infrastructure**: Defined in `/infra/core/container-apps.tf`
- **Environment Config**: Generated by `./env.sh`
- **Deployment**: Automated by `./deploy.sh`

## Infrastructure

The WebRTC signaling service is deployed as an Azure Container App with:

- **CPU**: 0.5 cores
- **Memory**: 1 GB
- **Replicas**: 1-5 (auto-scaling)
- **Port**: 3000
- **Ingress**: External (HTTPS)
- **Transport**: HTTP (supports WebSocket upgrades)

### Environment Variables (Managed by Terraform)

```hcl
PORT=3000
NODE_ENV=production
AZURE_TENANT_ID=<from-terraform>
AZURE_CLIENT_ID=<from-terraform>
ALLOWED_ORIGINS=<from-terraform>
```

## Deployment Steps

### 1. Apply Terraform Infrastructure

```bash
cd infra/core
terraform init -backend-config=backends/backend-dev.hcl
terraform apply -var-file=vars/dev.tfvars
```

This creates:
- Container App: `ca-<prefix>-webrtc-signaling-<env>`
- Exposes outputs: `webrtc_signaling_service_name`, `webrtc_signaling_service_url`, `webrtc_signaling_service_fqdn`

### 2. Generate Environment Configuration

```bash
cd services/webrtc-signaling
./env.sh
```

This generates `.env` with:
- Azure AD tenant/client IDs
- Allowed origins (UI service + App Gateway)
- Production configuration

### 3. Deploy the Service

```bash
./deploy.sh
```

This:
1. Builds Docker image for `linux/amd64`
2. Pushes to Azure Container Registry
3. Updates Container App with new image
4. Sets environment variables from `.env`

### 4. Update UI Configuration

```bash
cd services/ui
./env.sh
```

This adds `VITE_WEBRTC_SIGNALING_URL` to the UI's `.env` file.

### 5. Redeploy UI

```bash
./deploy.sh
```

## Accessing the Service

### Internal (Container Apps Environment)

```
https://<webrtc-signaling-fqdn>
```

Example:
```
https://ca-core-webrtc-signaling-dev.kindground-12345678.eastus.azurecontainerapps.io
```

### Public (via Application Gateway)

If you add routing rules to the Application Gateway:

```
https://<app-gateway-ip>/webrtc/
```

**Note**: You'll need to add backend pool and routing rules in `application-gateway.tf` for public access.

## Health Checks

Test the service is running:

```bash
# Internal
curl https://<webrtc-signaling-fqdn>/health

# Public (if configured)
curl https://<app-gateway-ip>/webrtc/health
```

Expected response:
```json
{
  "status": "healthy",
  "rooms": 0,
  "users": 0,
  "timestamp": "2025-10-21T10:00:00.000Z"
}
```

## Monitoring

### View Logs

```bash
az containerapp logs show \
  --name ca-<prefix>-webrtc-signaling-<env> \
  --resource-group rg-core-<env> \
  --follow
```

### Check Service Status

```bash
az containerapp show \
  --name ca-<prefix>-webrtc-signaling-<env> \
  --resource-group rg-core-<env> \
  --query "{name:name, status:properties.runningStatus, replicas:properties.template.scale, fqdn:properties.configuration.ingress.fqdn}"
```

### View Metrics

Navigate to the Container App in Azure Portal to see:
- Request count
- Response times
- CPU/Memory usage
- Replica count
- HTTP errors

## Environment-Specific Configuration

### Development

```bash
# Generate config for dev
./env.sh

# Deploy to dev environment
./deploy.sh
```

### Staging

```bash
# Switch Terraform workspace/backend
cd infra/core
terraform init -backend-config=backends/backend-stag.hcl
terraform workspace select stag || terraform workspace new stag

# Apply infrastructure
terraform apply -var-file=vars/stag.tfvars

# Generate config for staging
cd ../../services/webrtc-signaling
./env.sh

# Deploy
./deploy.sh
```

## Scaling

The Container App auto-scales between 1-5 replicas based on:
- HTTP request load
- CPU/Memory usage

To adjust scaling:

```bash
az containerapp update \
  --name ca-<prefix>-webrtc-signaling-<env> \
  --resource-group rg-core-<env> \
  --min-replicas 2 \
  --max-replicas 10
```

Or update in `container-apps.tf`:

```hcl
template {
  min_replicas = 2
  max_replicas = 10
}
```

## CORS Configuration

Allowed origins are automatically configured to include:
- UI Service FQDN
- Application Gateway IP
- Localhost (for development)

Format: Comma-separated list
```
https://ui-service.azurecontainerapps.io,https://52.123.45.67,http://localhost:5173
```

## Troubleshooting

### Service not starting

Check logs:
```bash
az containerapp logs show \
  --name ca-<prefix>-webrtc-signaling-<env> \
  --resource-group rg-core-<env> \
  --follow
```

### Authentication errors

Verify environment variables:
```bash
az containerapp show \
  --name ca-<prefix>-webrtc-signaling-<env> \
  --resource-group rg-core-<env> \
  --query "properties.template.containers[0].env"
```

### CORS errors

Check `ALLOWED_ORIGINS` includes your client origin with correct protocol and port.

### Connection timeouts

Ensure the Container App's ingress timeout is sufficient for WebSocket connections:

```bash
az containerapp ingress update \
  --name ca-<prefix>-webrtc-signaling-<env> \
  --resource-group rg-core-<env> \
  --transport http \
  --allow-insecure false
```

## Security Considerations

1. **JWT Validation**: Always enabled in production (`SKIP_TOKEN_VERIFICATION=false`)
2. **CORS**: Restrict to known origins only
3. **HTTPS Only**: Enforce secure connections
4. **Managed Identity**: Consider using for Azure AD authentication
5. **Network Isolation**: Container App is in internal VNet

## Next Steps

1. **Add Application Gateway Rules**: For public access via App Gateway
2. **Add Monitoring**: Set up Application Insights alerts
3. **Add TURN Server**: For better connectivity through firewalls/NAT
4. **Add Recording**: Integrate Azure Media Services for session recording
5. **Add Analytics**: Track room usage, connection quality metrics
